
# input_path = /workspace/0809_test/bangtransformer/test/magicmind/neox_20B_check_tokens/input.txt
# input_path = input.txt

mock_input_batch_size = 1 # only used when input_path is not given
mock_input_seq_len = 8 # only used when input_path is not given
once_batch_size = 4 # how many batches to run in one iteration. if input batch is smaller than this, will use input batch size

# filter_path = /data/AE/llm/models/neox_1.3b/tp_4/dump_weights/ # if not given or empty, will use mock filter (dirty data)
# filter_path = /opt/data/chenpeng/GLM_130B_BT_tp16_fp16/
filter_path = /projs/cnnl/cuixingxin/filters/glm/130B/int8/bin/
# filter_path = /opt/data/chenpeng/tp16_int8/

output_size = 0
cache_memory_len = 2048 # if not set, will use max_input_seq_len + output_size + 1
#cache_memory_len = 514 # if not set, will use max_input_seq_len + output_size + 1

# TODO: generate the position embedding table in C++ according to input scale
# if not set, they will be set same as cache_memory_len
position_embedding_seq_len_q = 2048
position_embedding_seq_len_k = 2048
